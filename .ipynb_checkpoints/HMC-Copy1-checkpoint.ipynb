{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from cycler import cycler\n",
    "import itertools\n",
    "import time\n",
    "import emcee\n",
    "import corner\n",
    "import copy\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd.scipy.misc import logsumexp\n",
    "import numpy as onp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "rc(\"font\", family=\"serif\", size=14)\n",
    "rc(\"text\", usetex=False)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['patch.linewidth'] = 2\n",
    "matplotlib.rcParams['axes.prop_cycle'] =\\\n",
    "    cycler(\"color\", ['k', 'c', 'm', 'y'])\n",
    "matplotlib.rcParams['axes.labelsize'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data $\\pmb{x}$ for a multivariate normal distribution $\\mathcal{N}(\\pmb{\\beta},\\Sigma)$, with covariance matrix $\\Sigma = \\sigma^2 I_D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "betas = np.linspace(0.5,5.0,num=dim)  # Vector containing the Means of the MVN\n",
    "sigma = 0.2\n",
    "cov = np.identity(dim)*(sigma**2)     # Covariance Matrix \n",
    "\n",
    "nobj = 2000                           # Number of Data Points\n",
    "L = np.linalg.cholesky(cov)           # Performs a Cholesky decomposition on Covariance Matrix\n",
    "xis = L @ np.random.randn(dim,nobj)   # Generates random samples Lv, where v ~ N(0,1) \n",
    "\n",
    "for i in range(nobj):\n",
    "    xis[:,i] += betas                 # Generates data x = Lv + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define functions necessary for the HMC procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lngaussian(x, mu, sig):\n",
    "    \"\"\"log of a normalized Gaussian\"\"\"\n",
    "    return - 0.5*((x - mu)/sig)**2 - 0.5*np.log(2*np.pi) - np.log(sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradmulngauss(x,mu,sig):\n",
    "    \"\"\"derative wrt mu of the log Gaussian\"\"\"\n",
    "    return (x-mu)/(sig**2)\n",
    "\n",
    "def gradsigmalngauss(x,mu,sig):\n",
    "    \"\"\"derative wrt sigma of the log Gaussian\"\"\"\n",
    "    return ((x - mu)**2)/(sig**3) - (1/sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a multivariate normal distribution with covariance matrix $\\Sigma = \\sigma^2 I_D$ can be written as the product of univariate normals, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} e^{-\\frac{1}{2}(\\pmb{x}-\\pmb{\\mu})^T\\Sigma^{-1}(\\pmb{x}-\\pmb{\\mu})} = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x_1-\\mu_1)^2/(2\\sigma^2)} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x_2-\\mu_2)^2/(2\\sigma^2)}\\cdot \\ ...\n",
    "$$\n",
    "\n",
    "This means that the log of this becomes\n",
    "$$\n",
    "-\\frac{D}{2}\\text{log}(2\\pi)-D\\text{log}(\\sigma)-\\frac{(x_1-\\mu_1)^2}{2\\sigma^2}-\\frac{(x_2-\\mu_2)^2}{2\\sigma^2}-...\n",
    "$$\n",
    "\n",
    "We will use this fact below to compute the gradient of the logarithm of the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(params):\n",
    "    betas = params[0:dim]\n",
    "    sigma = params[dim]\n",
    "\n",
    "    lnlikes = lngaussian(xis,betas.T[:,None],sigma)   # Computes the log likelihood\n",
    "    res = -np.sum(logsumexp(lnlikes, axis=1))\n",
    "\n",
    "    if ~np.isfinite(res):\n",
    "        print(\"Infinite likelihood call\")\n",
    "    return res\n",
    "\n",
    "def lnprob_grad(params):\n",
    "    betas = params[0:dim]\n",
    "    sigma = params[dim]\n",
    "    \n",
    "    #lnlikes = lngaussian(xis,betas.T[:,None],sigma) # Computes the log likelihood\n",
    "    #lnlikestot = np.sum(lnlikes,axis=0)\n",
    "    \n",
    "    grads = gradmulngauss(xis, betas.T[:,None],sigma) # Contains d log(lik)/d mu\n",
    "    grads2 = gradsigmalngauss(xis, betas.T[:,None],sigma)  # Contains d log(lik)/d sigma\n",
    "\n",
    "    subgrads = -np.sum(grads,axis=1)\n",
    "    subgrads2 = -np.sum(grads2)\n",
    "    subgrads = np.append(subgrads,subgrads2)\n",
    "\n",
    "    # Code below computes the derivative of logsumexp(log(lik)) wrt mu (subgrads) and sigma (subgrads2)\n",
    "    # This derivative is a softmax function. We apply the Exp-normalize trick to this.\n",
    "    #esumlnlikes = np.exp(np.sum(lnlikes,axis=0)-np.max(np.sum(lnlikes,axis=0)))\n",
    "    #subgrads = -np.dot(grads,esumlnlikes)\n",
    "    #subgrads2 = -np.sum(np.dot(grads2,esumlnlikes))   \n",
    "    #subgrads = (1/np.sum(np.exp(lnlikestot-np.max(lnlikestot))))*subgrads\n",
    "    \n",
    "    if np.any(~np.isfinite(subgrads)):\n",
    "        print(\"Infinite likelihood gradient\")# call with\", params)\n",
    "    if np.any(np.isnan(subgrads)):\n",
    "        print(\"NaN likelihood gradient\")# call with\", params)\n",
    "        \n",
    "    return subgrads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the diagonal of the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad2ndsigmalngauss(x,mu,sig):  \n",
    "    \"\"\"2nd derative wrt sigma of the log Gaussian\"\"\"\n",
    "    return -3*((x - mu)**2)/(sig**4) + (1/(sig**2))\n",
    "\n",
    "def lnprob_hessian(params):\n",
    "    betas = params[0:dim]\n",
    "    sigma = params[dim]\n",
    "    \n",
    "#     lnlikes = lngaussian(xis,betas.T[:,None],sigma)\n",
    "#     lnlikestot = np.sum(lnlikes,axis=0)\n",
    "    \n",
    "#     grads = gradmulngauss(xis, betas.T[:,None],sigma) # d log(lik)/d mu\n",
    "#     grads2 = gradsigmalngauss(xis, betas.T[:,None],sigma) # d log(lik)/d sigma\n",
    "    \n",
    "#     # Code below computes the 2nd derivative of logsumexp(log(lik)) wrt mu (grads2ndmu) and sigma (grads2ndsigma)\n",
    "#     esumlnlikes = np.exp(np.sum(lnlikes,axis=0)-np.max(np.sum(lnlikes,axis=0)))\n",
    "#     norm = np.sum(np.exp(lnlikestot-np.max(lnlikestot)))\n",
    "    \n",
    "#     grads2ndmu = -(1/norm)*(np.dot(grads**2,esumlnlikes)-np.dot(gradsmu,esumlnlikes))+(1/(norm**2))*((np.dot(grads,esumlnlikes))**2)\n",
    "#     grads2ndsigma = np.sum(-(1/norm)*(np.dot(grads2**2,esumlnlikes)+np.dot(gradssigma,esumlnlikes)) +(1/(norm**2))*(np.dot(grads2,esumlnlikes)**2))\n",
    "    \n",
    "    \n",
    "    grads2ndmu = np.ones((dim,1))/(sigma**2)  # This is the 2nd derivative of log(lik) wrt mu\n",
    "    grads2ndsigma = grad2ndsigmalngauss(xis, betas.T[:,None],sigma) # This is the 2nd derivative of log(lik) wrt sigma\n",
    "    \n",
    "    \n",
    "    grads2ndsigma = np.sum(grads2ndsigma)\n",
    "    diaghessian = np.append(grads2ndmu,grads2ndsigma)\n",
    "\n",
    "    if np.any(~np.isfinite(diaghessian)):\n",
    "        print(\"Infinite hessian\") # with\", params)\n",
    "    if np.any(np.isnan(diaghessian)):\n",
    "        print(\"NaN hessian\")# with\", params)\n",
    "\n",
    "    return diaghessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the HMC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates one HMC sample with \"num_steps\" steps of size \"step_size\" \n",
    "# x0 is the initial point, lnprob the posterior distribution, lnprobgrad are gradients.\n",
    "def hmc_sampler(x0, lnprob, lnprobgrad, step_size,\n",
    "                num_steps, inv_mass_matrix_diag=None, bounds=None, kwargs={}):\n",
    "    \n",
    "    inv_mass_matrix_diag_sqrt = inv_mass_matrix_diag**0.5\n",
    "    \n",
    "    v0 = np.random.randn(x0.size) / inv_mass_matrix_diag_sqrt\n",
    "    v = v0 - 0.5 * step_size * lnprobgrad(x0, **kwargs)\n",
    "    \n",
    "    #Check if got infinite of NaN values for lnprobgrad(x0, **kwargs)\n",
    "    if(np.any(~np.isfinite(v))):\n",
    "        return x0\n",
    "    if(np.any(np.isnan(v))):\n",
    "        return x0\n",
    "        \n",
    "    x = x0 + step_size * v * inv_mass_matrix_diag\n",
    "    \n",
    "    # Check if within bounds\n",
    "    ind_upper = x > bounds[:, 1]\n",
    "    x[ind_upper] = 2*bounds[ind_upper, 1] - x[ind_upper]\n",
    "    v[ind_upper] = - v[ind_upper]\n",
    "    ind_lower = x < bounds[:, 0]\n",
    "    x[ind_lower] = 2*bounds[ind_lower, 0] - x[ind_lower]\n",
    "    v[ind_lower] = - v[ind_lower]\n",
    "    ind_upper = x > bounds[:, 1]\n",
    "    ind_lower = x < bounds[:, 0]\n",
    "    ind_bad = np.logical_or(ind_lower, ind_upper)\n",
    "    if ind_bad.sum() > 0:\n",
    "        print('Error: could not confine samples within bounds!')\n",
    "        print('Number of problematic parameters:', ind_bad.sum(), \n",
    "              'out of', ind_bad.size)\n",
    "        return x0\n",
    "    \n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        v = v - step_size * lnprobgrad(x, **kwargs)\n",
    "        x = x + step_size * v * inv_mass_matrix_diag\n",
    "        \n",
    "        ind_upper = x > bounds[:, 1]\n",
    "        x[ind_upper] = 2*bounds[ind_upper, 1] - x[ind_upper]\n",
    "        v[ind_upper] = - v[ind_upper]\n",
    "        ind_lower = x < bounds[:, 0]\n",
    "        x[ind_lower] = 2*bounds[ind_lower, 0] - x[ind_lower]\n",
    "        v[ind_lower] = - v[ind_lower]\n",
    "        ind_upper = x > bounds[:, 1]\n",
    "        ind_lower = x < bounds[:, 0]\n",
    "        ind_bad = np.logical_or(ind_lower, ind_upper)\n",
    "        if ind_bad.sum() > 0:\n",
    "#             print('Error: could not confine samples within bounds!')\n",
    "#             print('Number of problematic parameters:', ind_bad.sum(), \n",
    "#                   'out of', ind_bad.size)\n",
    "            return x0\n",
    "\n",
    "    v = v - 0.5 * step_size * lnprobgrad(x, **kwargs)\n",
    "    \n",
    "    if(np.any(~np.isfinite(v))):\n",
    "        return x0\n",
    "    if(np.any(np.isnan(v))):\n",
    "        return x0\n",
    "    \n",
    "    orig = lnprob(x0, **kwargs)\n",
    "    current = lnprob(x, **kwargs)\n",
    "    \n",
    "    orig += 0.5 * np.sum(inv_mass_matrix_diag * v0**2.)\n",
    "    current += 0.5 * np.sum(inv_mass_matrix_diag * v**2.)\n",
    "        \n",
    "    p_accept = min(1.0, np.exp(orig - current))\n",
    "    if(np.any(~np.isfinite(x))):\n",
    "        print('Error: some parameters are infinite!', \n",
    "              np.sum(~np.isfinite(x)), 'out of', x.size)\n",
    "        print('HMC steps and stepsize:', num_steps, step_size)\n",
    "        return x0\n",
    "    if p_accept > np.random.uniform():\n",
    "        return x\n",
    "    else:\n",
    "        #if p_accept < 0.001:\n",
    "        #    print('Sample rejected due to small acceptance prob (', p_accept, ')')\n",
    "        #    print('HMC steps and stepsize:', num_steps, step_size)\n",
    "            #stop\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value of first 6 means [0.5        0.59183673 0.68367347 0.7755102  0.86734694]\n",
      "True value of sigma 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiffany/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "1-dimensional array given. Array must be at least two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-71f36cfa9f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mlnprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnprob_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     bounds=bounds, inv_mass_matrix_diag=np.linalg.inv(hess))\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mparam_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mburnin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \"\"\"\n\u001b[1;32m    525\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0m_assertRankAtLeast2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0m_assertNdSquareness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assertRankAtLeast2\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             raise LinAlgError('%d-dimensional array given. Array must be '\n\u001b[0;32m--> 204\u001b[0;31m                     'at least two-dimensional' % a.ndim)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assertSquareness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: 1-dimensional array given. Array must be at least two-dimensional"
     ]
    }
   ],
   "source": [
    "num_samples, burnin = 200000, 10000\n",
    "\n",
    "# True Parameters\n",
    "params = betas\n",
    "params = np.append(params,sigma)\n",
    "print(\"True value of first 6 means\", params[0:5])\n",
    "print(\"True value of sigma\", params[-1])\n",
    "\n",
    "#Bounds within which we confine our parameter samples\n",
    "bounds = np.zeros((params.size, 2))\n",
    "bounds[:, 0] = 0.0\n",
    "bounds[:, 1] = np.max(params)+3.0\n",
    "\n",
    "# Initial Parameter Guesses\n",
    "param_samples = np.zeros((num_samples, params.size))\n",
    "param_samples[0, :] = np.random.uniform(0,8,params.size) \n",
    "\n",
    "#hess = np.abs(lnprob_hessian(param_samples[0, :]))\n",
    "hess = np.ones(params.size)\n",
    "\n",
    "t1 = time.time() #Will time how long our HMC sampler takes to run\n",
    "for i in range(1, num_samples):\n",
    "\n",
    "    if i < 10000:\n",
    "        num_steps = np.random.randint(10,20) \n",
    "        step_size = np.random.uniform(1e-3,2e-2)\n",
    "    else:\n",
    "        num_steps = 10\n",
    "        step_size = np.random.uniform(1e-4,1e-2) #It ran well with (1e-4,1e-2)\n",
    "        \n",
    "    if i % 250 == 0:\n",
    "        #print(np.mean(param_samples[1:i-1,0:5],axis=0))\n",
    "        if i % 10000 == 0:\n",
    "            print(i, end=\" \")\n",
    "            print(\"Mean value of first 6 means so far\", np.mean(param_samples[5000:i-1,0:5],axis=0))\n",
    "            print(\"Variance in estimate 1st 6 means \", np.var(param_samples[5000:i-1,0:5],axis=0))\n",
    "            print(\"Mean value of sigma so far \",np.mean(param_samples[5000:i-1,-1],axis=0))\n",
    "            print(\"Variance in estimate sigma\",np.var(param_samples[5000:i-1,-1],axis=0))\n",
    "            \n",
    "        #Compute updated hessian for the mass matrix\n",
    "        newparams = np.mean(param_samples[0:i-1, :], axis=0)\n",
    "        hessold = copy.deepcopy(hess)\n",
    "        hess = np.abs(lnprob_hessian(newparams))\n",
    "        if np.any(~np.isfinite(hess)) or np.any(np.isnan(hess)):\n",
    "            hess = hessold\n",
    "    \n",
    "    # Run HMC sampler \n",
    "    param_samples[i, :] =\\\n",
    "        hmc_sampler(param_samples[i-1, :],\n",
    "                    lnprob, lnprob_grad,\n",
    "                    step_size, num_steps,\n",
    "                    bounds=bounds, inv_mass_matrix_diag=1/hess)\n",
    "\n",
    "param_samples = param_samples[burnin:, :]\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"It took \",t2-t1,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Autocorrelation Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how well our HMC sampler performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_names = [r'$\\beta_{'+str(i+1)+'}$' for i in range(dim)] +\\\n",
    "   [r'$\\sigma$' for i in range(1)]\n",
    "a = corner.corner(param_samples[:,0:5],color='k',truths=params, labels=param_names)\n",
    "plt.savefig(\"HMC_InferredMeans_51Parameters_200000Samples_Burnin10000_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corner.corner(param_samples[:,-5:], truths=params[-5:], labels=param_names[-5:])\n",
    "plt.savefig(\"HMC_InferredSigma_51Parameters_200000Samples_Burnin10000_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(param_samples[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HMC_samples_51Parameters_200000Samples.txt', 'w') as f:\n",
    "    for item in param_samples_hyperonly:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
