{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from cycler import cycler\n",
    "import itertools\n",
    "import time\n",
    "import emcee\n",
    "import corner\n",
    "import copy\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd.scipy.misc import logsumexp\n",
    "import numpy as onp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "rc(\"font\", family=\"serif\", size=14)\n",
    "rc(\"text\", usetex=False)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['patch.linewidth'] = 2\n",
    "matplotlib.rcParams['axes.prop_cycle'] =\\\n",
    "    cycler(\"color\", ['k', 'c', 'm', 'y'])\n",
    "matplotlib.rcParams['axes.labelsize'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data $\\pmb{x}$ for a multivariate normal distribution $\\mathcal{N}(\\pmb{\\beta},\\Sigma)$, with covariance matrix $\\Sigma = \\sigma^2 I_D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "betas = np.linspace(0.5,5.0,num=dim)  # Vector containing the Means of the MVN\n",
    "sigma = 0.2\n",
    "cov = np.identity(dim)*(sigma**2)     # Covariance Matrix \n",
    "\n",
    "nobj = 2000                           # Number of Data Points\n",
    "L = np.linalg.cholesky(cov)           # Performs a Cholesky decomposition on Covariance Matrix\n",
    "xis = L @ np.random.randn(dim,nobj)   # Generates random samples Lv, where v ~ N(0,1) \n",
    "\n",
    "for i in range(nobj):\n",
    "    xis[:,i] += betas                 # Generates data x = Lv + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define functions necessary for the HMC procedure\n",
    "\n",
    "Note that a multivariate normal distribution with covariance matrix $\\Sigma = \\sigma^2 I_D$ can be written as the product of univariate normals, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{(2\\pi)^D|\\Sigma|}} e^{-\\frac{1}{2}(\\pmb{x}-\\pmb{\\mu})^T\\Sigma^{-1}(\\pmb{x}-\\pmb{\\mu})} = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x_1-\\mu_1)^2/(2\\sigma^2)} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x_2-\\mu_2)^2/(2\\sigma^2)}\\cdot \\ ...\n",
    "$$\n",
    "\n",
    "This means that the log of this becomes\n",
    "$$\n",
    "-\\frac{D}{2}\\text{log}(2\\pi)-D\\text{log}(\\sigma)-\\frac{(x_1-\\mu_1)^2}{2\\sigma^2}-\\frac{(x_2-\\mu_2)^2}{2\\sigma^2}-...\n",
    "$$\n",
    "\n",
    "We will use this fact below to compute the gradient of the logarithm of the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lngaussian(x, mu, sig):\n",
    "    \"\"\"log of a normalized Gaussian\"\"\"\n",
    "    return - 0.5*((x - mu)/sig)**2 - 0.5*np.log(2*np.pi) - np.log(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradmulngauss(x,mu,sig):\n",
    "    \"\"\"derative wrt mu of the log Gaussian\"\"\"\n",
    "    return (x-mu)/(sig**2)\n",
    "\n",
    "def gradsigmalngauss(x,mu,sig):\n",
    "    \"\"\"derative wrt sigma of the log Gaussian\"\"\"\n",
    "    return ((x - mu)**2)/(sig**3) - (1/sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(params):\n",
    "    betas = params[0:dim]\n",
    "    sigma = params[dim]\n",
    "\n",
    "    lnlikes = lngaussian(xis,betas.T[:,None],sigma)   # Computes the log likelihood\n",
    "    res = - np.sum(logsumexp(lnlikes, axis=1))\n",
    "\n",
    "    if ~np.isfinite(res):\n",
    "        print(\"Infinite likelihood call\")\n",
    "    return res\n",
    "\n",
    "def lnprob_grad(params):\n",
    "    betas = params[0:dim]\n",
    "    sigma = params[dim]\n",
    "    \n",
    "    lnlikes = lngaussian(xis,betas.T[:,None],sigma) # Computes the log likelihood\n",
    "    lnlikestot = np.sum(lnlikes,axis=0)\n",
    "    \n",
    "    grads = gradmulngauss(xis, betas.T[:,None],sigma) # Contains d log(lik)/d mu\n",
    "    grads2 = gradsigmalngauss(xis, betas.T[:,None],sigma)  # Contains d log(lik)/d sigma\n",
    "    \n",
    "    # Code below computes the derivative of logsumexp(log(lik)) wrt mu (subgrads) and sigma (subgrads2)\n",
    "    # This derivative is a softmax function. We apply the Exp-normalize trick to this.\n",
    "    esumlnlikes = np.exp(np.sum(lnlikes,axis=0)-np.max(np.sum(lnlikes,axis=0)))\n",
    "    subgrads = -np.dot(grads,esumlnlikes)\n",
    "    subgrads2 = -np.sum(np.dot(grads2,esumlnlikes))   \n",
    "\n",
    "    subgrads = np.append(subgrads,subgrads2)\n",
    "\n",
    "    subgrads = (1/np.sum(np.exp(lnlikestot-np.max(lnlikestot))))*subgrads\n",
    "    \n",
    "    if np.any(~np.isfinite(subgrads)):\n",
    "        print(\"Infinite likelihood gradient\")# call with\", params)\n",
    "    if np.any(np.isnan(subgrads)):\n",
    "        print(\"NaN likelihood gradient\")# call with\", params)\n",
    "        \n",
    "    return subgrads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the diagonal of the Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad2ndsigmalngauss(x,mu,sig):  \n",
    "    \"\"\"2nd derative wrt sigma of the log Gaussian\"\"\"\n",
    "    return -3*((x - mu)**2)/(sig**4) + (1/(sig**2))\n",
    "\n",
    "def lnprob_hessian(params):\n",
    "    betas = params[0:dim]\n",
    "    sigma = params[dim]\n",
    "    \n",
    "    lnlikes = lngaussian(xis,betas.T[:,None],sigma)\n",
    "    lnlikestot = np.sum(lnlikes,axis=0)\n",
    "    \n",
    "    grads = gradmulngauss(xis, betas.T[:,None],sigma) # d log(lik)/d mu\n",
    "    grads2 = gradsigmalngauss(xis, betas.T[:,None],sigma) # d log(lik)/d sigma\n",
    "    \n",
    "    gradsmu = np.ones(np.shape(grads))/(sigma**2)  # This is the 2nd derivative of log(lik) wrt mu\n",
    "    gradssigma = grad2ndsigmalngauss(xis, betas.T[:,None],sigma) # This is the 2nd derivative of log(lik) wrt sigma\n",
    "    \n",
    "    # Code below computes the 2nd derivative of logsumexp(log(lik)) wrt mu (grads2ndmu) and sigma (grads2ndsigma)\n",
    "    esumlnlikes = np.exp(np.sum(lnlikes,axis=0)-np.max(np.sum(lnlikes,axis=0)))\n",
    "    norm = np.sum(np.exp(lnlikestot-np.max(lnlikestot)))\n",
    "    \n",
    "    grads2ndmu = -(1/norm)*(np.dot(grads**2,esumlnlikes)-np.dot(gradsmu,esumlnlikes))+(1/(norm**2))*((np.dot(grads,esumlnlikes))**2)\n",
    "    grads2ndsigma = np.sum(-(1/norm)*(np.dot(grads2**2,esumlnlikes)+np.dot(gradssigma,esumlnlikes)) +(1/(norm**2))*(np.dot(grads2,esumlnlikes)**2))\n",
    "\n",
    "    diaghessian = np.append(grads2ndmu,grads2ndsigma)\n",
    "\n",
    "    if np.any(~np.isfinite(diaghessian)):\n",
    "        print(\"Infinite hessian\") \n",
    "    if np.any(np.isnan(diaghessian)):\n",
    "        print(\"NaN hessian\")\n",
    "\n",
    "    return diaghessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the HMC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates one HMC sample with \"num_steps\" steps of size \"step_size\" \n",
    "# x0 is the initial point, lnprob the posterior distribution, lnprobgrad are gradients.\n",
    "def hmc_sampler(x0, lnprob, lnprobgrad, step_size,\n",
    "                num_steps, inv_mass_matrix_diag=None, bounds=None, kwargs={}):\n",
    "\n",
    "    inv_mass_matrix_diag_sqrt = inv_mass_matrix_diag**0.5\n",
    "    \n",
    "    v0 = np.random.randn(x0.size) / inv_mass_matrix_diag_sqrt\n",
    "    v = v0 - 0.5 * step_size * lnprobgrad(x0, **kwargs)\n",
    "    \n",
    "    #Check if got infinite of NaN values for lnprobgrad(x0, **kwargs)\n",
    "    if(np.any(~np.isfinite(v))):\n",
    "        return x0\n",
    "    if(np.any(np.isnan(v))):\n",
    "        return x0\n",
    "        \n",
    "    x = x0 + step_size * v * inv_mass_matrix_diag\n",
    "    \n",
    "    # Check if within bounds\n",
    "    ind_upper = x > bounds[:, 1]\n",
    "    x[ind_upper] = 2*bounds[ind_upper, 1] - x[ind_upper]\n",
    "    v[ind_upper] = - v[ind_upper]\n",
    "    ind_lower = x < bounds[:, 0]\n",
    "    x[ind_lower] = 2*bounds[ind_lower, 0] - x[ind_lower]\n",
    "    v[ind_lower] = - v[ind_lower]\n",
    "    ind_upper = x > bounds[:, 1]\n",
    "    ind_lower = x < bounds[:, 0]\n",
    "    ind_bad = np.logical_or(ind_lower, ind_upper)\n",
    "    if ind_bad.sum() > 0:\n",
    "        print('Error: could not confine samples within bounds!')\n",
    "        print('Number of problematic parameters:', ind_bad.sum(), \n",
    "              'out of', ind_bad.size)\n",
    "        return x0\n",
    "    \n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        v = v - step_size * lnprobgrad(x, **kwargs)\n",
    "        x = x + step_size * v * inv_mass_matrix_diag\n",
    "        \n",
    "        ind_upper = x > bounds[:, 1]\n",
    "        x[ind_upper] = 2*bounds[ind_upper, 1] - x[ind_upper]\n",
    "        v[ind_upper] = - v[ind_upper]\n",
    "        ind_lower = x < bounds[:, 0]\n",
    "        x[ind_lower] = 2*bounds[ind_lower, 0] - x[ind_lower]\n",
    "        v[ind_lower] = - v[ind_lower]\n",
    "        ind_upper = x > bounds[:, 1]\n",
    "        ind_lower = x < bounds[:, 0]\n",
    "        ind_bad = np.logical_or(ind_lower, ind_upper)\n",
    "        if ind_bad.sum() > 0:\n",
    "            print('Error: could not confine samples within bounds!')\n",
    "            print('Number of problematic parameters:', ind_bad.sum(), \n",
    "                  'out of', ind_bad.size)\n",
    "            return x0\n",
    "\n",
    "    v = v - 0.5 * step_size * lnprobgrad(x, **kwargs)\n",
    "    \n",
    "    if(np.any(~np.isfinite(v))):\n",
    "        return x0\n",
    "    if(np.any(np.isnan(v))):\n",
    "        return x0\n",
    "    \n",
    "    orig = lnprob(x0, **kwargs)\n",
    "    current = lnprob(x, **kwargs)\n",
    "    \n",
    "    orig += 0.5 * np.sum(inv_mass_matrix_diag * v0**2.)\n",
    "    current += 0.5 * np.sum(inv_mass_matrix_diag * v**2.)\n",
    "        \n",
    "    p_accept = min(1.0, np.exp(orig - current))\n",
    "    if(np.any(~np.isfinite(x))):\n",
    "        print('Error: some parameters are infinite!', \n",
    "              np.sum(~np.isfinite(x)), 'out of', x.size)\n",
    "        print('HMC steps and stepsize:', num_steps, step_size)\n",
    "        return x0\n",
    "    if p_accept > np.random.uniform():\n",
    "        return x\n",
    "    else:\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True value of first 6 means [0.5        0.59183673 0.68367347 0.7755102  0.86734694]\n",
      "True value of sigma 0.2\n",
      "10000 Mean value of first 6 means so far [0.78722745 0.59606016 0.82947357 0.85299883 0.81786084]\n",
      "Variance in estimate 1st 6 means  [0.11117857 0.12092276 0.10997273 0.10225646 0.06534765]\n",
      "Mean value of sigma so far  0.24810134013585347\n",
      "Variance in estimate sigma 0.010070606082323423\n",
      "20000 Mean value of first 6 means so far [0.64404997 0.62767997 0.78708447 0.88368973 0.89056374]\n",
      "Variance in estimate 1st 6 means  [0.09082566 0.08509203 0.1461388  0.0826338  0.05825275]\n",
      "Mean value of sigma so far  0.24136849315301792\n",
      "Variance in estimate sigma 0.006233184988314298\n",
      "30000 Mean value of first 6 means so far [0.61974365 0.66848201 0.73221942 0.81058798 0.83776665]\n",
      "Variance in estimate 1st 6 means  [0.05928591 0.06971292 0.10149753 0.06830492 0.04815078]\n",
      "Mean value of sigma so far  0.20237967950891708\n",
      "Variance in estimate sigma 0.0063061131249919095\n",
      "40000 Mean value of first 6 means so far [0.61693735 0.65471977 0.73545846 0.77786321 0.82644303]\n",
      "Variance in estimate 1st 6 means  [0.06391996 0.0762163  0.09010331 0.11747843 0.05316819]\n",
      "Mean value of sigma so far  0.23592372689377658\n",
      "Variance in estimate sigma 0.010555686683053915\n"
     ]
    }
   ],
   "source": [
    "num_samples, burnin = 200000, 10000\n",
    "\n",
    "# True Parameters\n",
    "params = betas\n",
    "params = np.append(params,sigma)\n",
    "print(\"True value of first 6 means\", params[0:5])\n",
    "print(\"True value of sigma\", params[-1])\n",
    "\n",
    "#Bounds within which we confine our parameter samples\n",
    "bounds = np.zeros((params.size, 2))\n",
    "bounds[:, 0] = 0.0\n",
    "bounds[:, 1] = np.max(params)+3.0\n",
    "\n",
    "# Initial Parameter Guesses\n",
    "param_samples = np.zeros((num_samples, params.size))\n",
    "param_samples[0, :] = np.random.uniform(0,8,params.size) \n",
    "\n",
    "#hess = np.abs(lnprob_hessian(param_samples[0, :]))\n",
    "hess = np.ones(params.size)\n",
    "\n",
    "t1 = time.time() #Will time how long our HMC sampler takes to run\n",
    "for i in range(1, num_samples):\n",
    "\n",
    "    if i < 10000:\n",
    "        num_steps = np.random.randint(10,20) \n",
    "        step_size = np.random.uniform(1e-3,2e-2)\n",
    "    else:\n",
    "        num_steps = 10\n",
    "        step_size = np.random.uniform(1e-4,1e-2) #It ran well with (1e-4,1e-2)\n",
    "        \n",
    "    if i % 250 == 0:\n",
    "        if i % 10000 == 0:\n",
    "            print(i, end=\" \")\n",
    "            print(\"Mean value of first 6 means so far\", np.mean(param_samples[5000:i-1,0:5],axis=0))\n",
    "            print(\"Variance in estimate 1st 6 means \", np.var(param_samples[5000:i-1,0:5],axis=0))\n",
    "            print(\"Mean value of sigma so far \",np.mean(param_samples[5000:i-1,-1],axis=0))\n",
    "            print(\"Variance in estimate sigma\",np.var(param_samples[5000:i-1,-1],axis=0))\n",
    "            \n",
    "        #Compute updated hessian for the mass matrix\n",
    "        newparams = np.mean(param_samples[0:i-1, :], axis=0)\n",
    "        hessold = copy.deepcopy(hess)\n",
    "        hess = np.abs(lnprob_hessian(newparams))\n",
    "        if np.any(~np.isfinite(hess)) or np.any(np.isnan(hess)):\n",
    "            hess = hessold\n",
    "    \n",
    "    # Run HMC sampler \n",
    "    param_samples[i, :] =\\\n",
    "        hmc_sampler(param_samples[i-1, :],\n",
    "                    lnprob, lnprob_grad,\n",
    "                    step_size, num_steps,\n",
    "                    bounds=bounds, inv_mass_matrix_diag=1/hess)\n",
    "\n",
    "param_samples = param_samples[burnin:, :]\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"It took \",t2-t1,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how well our HMC sampler performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [r'$\\beta_{'+str(i+1)+'}$' for i in range(dim)] +\\\n",
    "   [r'$\\sigma$' for i in range(1)]\n",
    "a = corner.corner(param_samples[:,0:5],color='k',truths=params, labels=param_names)\n",
    "plt.savefig(\"HMC_InferredMeans_51Parameters_200000Samples_Burnin10000_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corner.corner(param_samples[:,-5:], truths=params[-5:], labels=param_names[-5:])\n",
    "plt.savefig(\"HMC_InferredSigma_51Parameters_200000Samples_Burnin10000_diff.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(param_samples[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HMC_samples_51Parameters_200000Samples.txt', 'w') as f:\n",
    "    for item in param_samples_hyperonly:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
